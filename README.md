# Observability AI Platform

A proof-of-concept project for observability and AI-driven log analysis with advanced reasoning capabilities.

This project demonstrates a practical approach to observability using AI techniques for log analysis. It provides tools to ingest, generate, and analyze logs, leveraging Retrieval-Augmented Generation (RAG) to enable advanced querying and insights from log data with intelligent reasoning and actionable recommendations.

## Key features:

- Automated log ingestion and tracking
- Synthetic log generation for testing and simulation
- RAG-based querying for intelligent log search and summarization
- **AI-powered reasoning engine** for root cause analysis and actionable recommendations
- **Structured output** with confidence scoring and evidence tracking
- Integration with vector databases (e.g., Qdrant) for efficient semantic search
- Multi-modal support (text, screenshots, hybrid queries)
- Comprehensive logging and monitoring capabilities
- Modular Python codebase for easy extension and experimentation

The project is ideal for experimenting with AI-driven observability workflows, building prototypes for log analytics, and exploring how LLMs and vector search can enhance traditional monitoring systems with intelligent reasoning capabilities.

## Project Structure

```
.
├── ingestracker/           # Tracks ingested log files
│   └── ingested_files.json
├── input-logs/             # Contains input log files for processing (Generated by log_generator.py)
├── lenv/                   # Python virtual environment (auto-generated)
├── logs/                   # Output or processed logs (currently empty)
├── src/                    # Source code for log ingestion, generation, and querying
│   ├── api.py              # (Optional) API endpoints for the project to generate sample logs and ingest them into Qdrant DB
│   ├── app.py              # Streamlit or main app entry point
│   ├── ingest_logs.py      # Ingesting logs into Qdrant DB
│   ├── log_generator.py    # Synthetic log generator
│   ├── rag_query_log.py    # RAG-based log querying with multi-modal support
│   ├── reasoning.py        # AI reasoning engine for root cause analysis and recommendations
├── requirements.txt        # Python dependencies
├── Dockerfile              # Docker file for containerization
├── docker-compose.yml      # Docker compose yaml for running the entire setup in local
```

## Architecture

![Alt text](images/arch.png)

## RAG Overview

![Alt text](images/rag-overview.png)

## Setup

1. **Run Qdrant DB in localhost**

   ```
   docker run -d -p 6333:6333 --name qdrant qdrant/qdrant
   ```

   - The Qdrant UI will be accessible at http://localhost:6333/dashboard
   - Or you can leverage Free tier offered by [Qdrant Cloud](https://qdrant.tech/pricing/)
   - Remember to use API Key along with URL while creating Qdrant client if you are using Qdrant Cloud
     ![Alt text](images/qdrant-ui.png)

2. **Clone the repository:**

   ```sh
   git clone <repo-url>
   cd observability-qdrant
   ```

3. **Create and activate a virtual environment (optional if `lenv/` is not used):**

   ```sh
   python -m venv lenv
   . lenv\Scripts\activate
   ```

4. **Install dependencies:**
   ```sh
   pip install -r requirements.txt
   ```

## Usage

- **Generate Sample logs (In VSCode Terminal):**

  ```sh
  python src/log-generator.py <YYYY-MM-DD> <Number of log entries you need per file>
  ```

- **Ingest logs into Qdrant (In VSCode Terminal):**

  ```sh
  python src/ingest-logs.py <Qdrant Collection Name>
  ```

- **Query logs using RAG (In VSC Code Terminal):**

  ```sh
  python src/rag_query_log.py --mode text --query "your question here"
  ```

- **Query logs with AI reasoning (In VSC Code Terminal):**

  ```sh
  python src/rag_query_log.py --mode reasoning --query "your question here"
  ```

- **Run both the apps (API and Streamlit UI) in local docker environment:**

  - This will start both FastAPI and Streamlit UI
  - You can access the apps in the corresponding URLs (shown below)
    ```sh
    docker-compose up
    ```

- **Running Swagger separately for generating and ingesting logs:**

  - Start the uvicorn server with api.py

  ```sh
  cd src
  uvicorn api:app --reload
  ```

  - This will start the FastAPI app at http://localhost:8000/docs
  - You can use the generate_logs and ingest_logs apis to do the tasks
  - Inputs:
    - generate_logs:
      - Date String: YYYY-MM-DD
    - Number of log entries needed: Any number. Default value is 2000
    - ingest_logs:
      - Collection Name: Unique Qdrant collection name
  - This uvicorn app can be closed or killed after the ingestion is done. It is used only for the Ingestion
    ![Alt text](images/log-api.png)

- **Running Streamlit Chat UI seprately:**
  - Start the Streamlit application for running the Chat UI
  ```sh
  cd src
  streamlit run app.py
  ```
  - This will open the Chat UI at http://localhost:8501
    ![Alt text](images/Streamlit-ui.png)

## Query Modes

The system supports multiple query modes:

- **Text Mode** (`--mode text`): Standard RAG-based querying
- **Reasoning Mode** (`--mode reasoning`): AI-powered analysis with root cause identification and actionable recommendations
- **Screenshot Mode** (`--mode screenshot`): Query using image/diagram descriptors
- **Hybrid Mode** (`--mode hybrid`): Combines text and screenshot search

### Additional Parameters

- `--k`: Number of results to retrieve (default: 5)
- `--threshold`: Similarity threshold for filtering results (default: 0.5)
- `--filters`: Comma-separated filters (e.g., `level=ERROR,namespace=namespace-3`)
- `--incidenthistory`: Include historical incident data in analysis

### Environment Variables

Create a `.env` file with the following variables:

```env
OPENAI_API_KEY=your_openai_api_key
QDRANT_URL=http://localhost:6333
QDRANT_CLOUD_API_KEY=your_qdrant_cloud_api_key  # Optional, for Qdrant Cloud
COLLECTION_NAME=aks_logs
COLLECTION_INCIDENTS=incidents
COLLECTION_SCREENSHOTS=log_shots
RETRIEVAL_MODEL=gpt-4o-mini
EMBEDDING_MODEL=text-embedding-3-small
```

## AI Reasoning Features

The reasoning engine provides:

- **Root Cause Analysis**: Identifies potential root causes based on log evidence
- **Actionable Recommendations**: Structured action items with priorities and ownership
- **Confidence Scoring**: Quantifies the reliability of the analysis
- **Evidence Tracking**: Links recommendations back to specific log entries
- **Insufficient Evidence Detection**: Flags when more data is needed for reliable analysis

### Reasoning Output Structure

```json
{
  "answer": "Detailed analysis of the issue",
  "root_cause": "Identified root cause hypothesis",
  "recommended_actions": [
    {
      "action": "Specific action to take",
      "why": "Reasoning behind the action",
      "priority": "P1/P2/P3",
      "owner": "Responsible team/person",
      "runbook": "Link to relevant runbook"
    }
  ],
  "confidence": 0.85,
  "insufficient_evidence": false,
  "evidence_doc_ids": ["doc1", "doc2"]
}
```

## Few questions you can ask

- **Cluster / Node Metrics:**
  - Which nodes in the aks-demo-cluster are experiencing high CPU usage?
  - What is the memory usage trend for aks-nodepool-2 over the last few hours?
  - Which pods are running on aks-nodepool-3 and what are their CPU usages?
  - How many nodes reported CPU usage above 2% between 00:00 and 03:00 on 2025-08-09?
- **Pod / Container Metrics:**
  - Which containers in namespace-1 had memory usage above 1000MB?
  - List the pods in namespace-4 that logged ERROR level messages.
  - What is the average CPU usage of app1-container across all its pods?
  - Show the logs of app7-pod-5 during the timeframe 00:28–01:12.
- **Log Level / Event Analysis:**
  - Which pods emitted WARN or ERROR messages on 2025-08-09?
  - Are there any DEBUG logs for app6-container?
  - How many INFO messages were logged for app10 on 2025-08-09?
  - What eventType events occurred, such as NodeScaledUp?
- **Message / Trace Analysis:**
  - Show logs where the message mentions “Node ScaledUp.”
  - List all trace IDs associated with ERROR level logs.
  - Which logs indicate potential resource pressure based on message content?
- **Cross-cutting / Comparative Queries:**
  - Compare CPU usage of app1-pod-5 and app7-pod-5.
  - Which pods had the highest memory usage in namespace-5?
  - Which nodes consistently show low CPU usage below 0.5%?
